[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mostly Monte Carlo Seminar Series",
    "section": "",
    "text": "A monthly series on the theory and practice of Monte Carlo in statistics and data science."
  },
  {
    "objectID": "index.html#seminar-details",
    "href": "index.html#seminar-details",
    "title": "Mostly Monte Carlo Seminar Series",
    "section": "Seminar Details",
    "text": "Seminar Details\nFrequency: 2nd Friday of each month (approximately)\nLocation: PariSanté Campus\nOrganisers: Andrea Bertazzi and Joshua Bon"
  },
  {
    "objectID": "index.html#join-the-mailing-list",
    "href": "index.html#join-the-mailing-list",
    "title": "Mostly Monte Carlo Seminar Series",
    "section": "Join the mailing list",
    "text": "Join the mailing list\n\n&lt;p&gt;Loading…&lt;/p&gt;"
  },
  {
    "objectID": "sem/s2024_04.html",
    "href": "sem/s2024_04.html",
    "title": "April 2024",
    "section": "",
    "text": "Date: 4PM Friday 12th April 2024\nRoom: Salle 06, PariSanté Campus\n\n\nTitle TBC\nKamélia Daudel ESSEC Business School\nAbstract TBC\n\n\nSpeaker 2 TBC"
  },
  {
    "objectID": "sem/s2024_01.html",
    "href": "sem/s2024_01.html",
    "title": "January 2024",
    "section": "",
    "text": "Date: 4PM Friday 12th January 2024\nRoom: Salle 07, PariSanté Campus\n\n\nCombining Normalizing Flows and Quasi-Monte Carlo\nCharly Andral Ceremade, Université Paris Dauphine-PSL\nRecent advances in machine learning have led to the development of new methods for enhancing Monte Carlo methods such as Markov chain Monte Carlo (MCMC) and importance sampling (IS). One such method is normalizing flows, which use a neural network to approximate a distribution by evaluating it pointwise. Normalizing flows have been shown to improve the performance of MCMC and IS. On the other side, (randomized) quasi-Monte Carlo methods are used to perform numerical integration. They replace the random sampling of Monte Carlo by a sequence which cover the hypercube more uniformly, resulting in better convergence rates for the error that plain Monte Carlo. In this work, we combine these two methods by using quasi-Monte Carlo to sample the initial distribution that is transported by the flow. We demonstrate through numerical experiments that this combination can lead to an estimator with significantly lower variance than if the flow was sampled with a classic Monte Carlo.\n\n\nOptimizing the diffusion of overdamped Langevin dynamics\nRégis Santet, CERMICS, École des Ponts & MATHERIALS, INRIA Paris\nOverdamped Langevin dynamics are reversible stochastic differential equations which are commonly used to sample probability measures in high dimensional spaces, such as the ones appearing in computational statistical physics and Bayesian inference. By varying the diffusion coefficient, there are in fact infinitely many reversible overdamped Langevin dynamics which preserve the target probability measure at hand. This suggests to optimize the diffusion coefficient in order to increase the convergence rate of the dynamics, as measured by the spectral gap of the generator associated with the stochastic differential equation. We analytically study this problem here, obtaining in particular necessary conditions on the optimal diffusion coefficient. We also derive an explicit expression of the optimal diffusion in some homogenized limit. Numerical results, both relying on discretizations of the spectral gap problem and Monte Carlo simulations of the stochastic dynamics, demonstrate the increased quality of the sampling arising from an appropriate choice of the diffusion coefficient.\nThis is joint work with Tony Lelièvre, Grigorios Pavliotis, Geneviève Robin and Gabriel Stoltz."
  },
  {
    "objectID": "sem/s2024_03.html",
    "href": "sem/s2024_03.html",
    "title": "March 2024",
    "section": "",
    "text": "Date: 4PM Friday 8th March 2024\nRoom: Salle 03, PariSanté Campus\n\n\nTransporting measures for sampling: parametric and non-parametric approaches inspired by generative modelling\nMarylou Gabrié CMAP, École Polytechnique\nGenerative models and statistical mechanics have a long history of cross-fertilization. Recently, it has been shown that generative models, such as normalizing flows, can assist sampling of metastable systems. This remarkable ability comes from the high-expressivity of generative models that can approach complex distributions while remaining tractable. However, the training accuracy of generative models deteriorates when the dimension and complexity of the target measure are pushed. Inspired by recent progress in generative modelling relying on stochastic processes, non-parametric sampling algorithms can also be derived to sample from metastable systems. Are non-parametric methods more scalable?\n\n\nImplicit Diffusion: Efficient Optimization through Stochastic Sampling\nPierre Marion École Polytechnique Fédérale de Lausanne\nWe present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings."
  },
  {
    "objectID": "sem/s2023_10.html",
    "href": "sem/s2023_10.html",
    "title": "October 2023",
    "section": "",
    "text": "Date: 4PM Friday 13th October 2023\nRoom: Salle 07, PariSanté Campus\n\n\nPiecewise deterministic sampling with splitting schemes\nAndrea Bertazzi CMAP, École Polytechnique \nPiecewise deterministic Markov processes (PDMPs) received substantial interest in recent years as an alternative to classical Markov chain Monte Carloalgorithms. While theoretical properties of PDMPs have been studied extensively, their practical implementation remains limited to specific applications in which bounds on the gradient of the negative log-target can be derived. In order to address this problem, we propose to approximate PDMPs using splitting schemes, that means simulating the deterministic dynamics and the random jumps in two different stages. We show that symmetric splittings of PDMPs are of second order. Then we focus on the Zig-Zag sampler (ZZS) and show how to remove the bias of the splitting scheme with a skew reversible Metropolis filter. Finally, we illustrate with numerical simulations the advantages of our proposed scheme over competitors.\n\n\nBayesian score calibration for approximate models\nJoshua Bon Ceremade, Université Paris Dauphine-PSL\nScientists continue to develop increasingly complex mechanistic models to reflect their knowledge more realistically. Statistical inference using these models can be challenging since the corresponding likelihood function is often intractable and model simulation may be computationally burdensome.  Fortunately, in many of these situations, it is possible to adopt a surrogate model or approximate likelihood function.  It may be convenient to base Bayesian inference directly on the surrogate, but this can result in bias and poor uncertainty quantification.  In this paper we propose a new method for adjusting approximate posterior samples to reduce bias and produce more accurate uncertainty quantification.  We do this by optimizing a transform of the approximate posterior that maximizes a scoring rule.  Our approach requires only a (fixed) small number of complex model simulations and is numerically stable.  We demonstrate good performance of the new method on several examples of increasing complexity."
  },
  {
    "objectID": "sem/s202X_XX.html",
    "href": "sem/s202X_XX.html",
    "title": "MONTH 2024",
    "section": "",
    "text": "Date: 4PM Friday XXth MONTH 2024\nRoom: Salle XX, PariSanté Campus\n\n\nTitle\nSpeaker Affiliation\nAbstract\n\n\nTitle\nSpeaker Affiliation\nAbstract"
  },
  {
    "objectID": "sem/s2023_11.html",
    "href": "sem/s2023_11.html",
    "title": "November 2023",
    "section": "",
    "text": "Date: 4PM Friday 17th November 2023\nRoom: Salle 06, PariSanté Campus\n\n\nMonte Carlo guided Diffusion models for Bayesian linear inverse problems\nYazid Janati El Idrissi CMAP, École Polytechnique\nIll-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems. In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The proposed algorithm, MCGDiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.\n\n\nGenerative Flow Networks as Entropy-Regularized RL\nDaniil Tiapkin CMAP, École Polytechnique\nThe recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the realm of generative flow networks."
  },
  {
    "objectID": "sem/s2023_12.html",
    "href": "sem/s2023_12.html",
    "title": "December 2023",
    "section": "",
    "text": "Date: 4PM Friday 15th December 2023\nRoom: Salle 08, PariSanté Campus\n\n\nSVBMC: Fast post-processing Bayesian inference with noisy evaluations of the likelihood\nGrégoire Clarté University of Helsinki, University of Edinburgh\nIn many cases, the exact likelihood is unavailable, and can only be accessed through a noisy and expensive process – for example, in Plasma Physics. Furthermore, Bayesian inference often comes in at a second moment, for example after running an optimization algorithm to find a MAP estimate. To tackle both these issues, we introduce Sparse Variational Bayesian Monte Carlo (SVBMC), a method for fast “post-processes” Bayesian inference for models with black-box and noisy likelihoods. SVBMC reuses all existing target density evaluations – for example, from previous optimizations or partial Markov Chain Monte Carlo runs – to build a sparse Gaussian process (GP) surrogate model of the log posterior density. Uncertain regions of the surrogate are then refined via active learning as needed. Our work builds on the Variational Bayesian Monte Carlo (VBMC) framework for sample-efficient inference, with several novel contributions. First, we make VBMC scalable to a large number of pre-existing evaluations via sparse GP regression, deriving novel Bayesian quadrature formulae and acquisition functions for active learning with sparse GPs. Second, we introduce noise shaping, a general technique to induce the sparse GP approximation to focus on high posterior density regions. Third, we prove theoretical results in support of the SVBMC refinement procedure. We validate our method on a variety of challenging synthetic scenarios and real-world applications. We find that SVBMC consistently builds good posterior approximations by post-processing of existing model evaluations from different sources, often requiring only a small number of additional density evaluations.\n\n\nVariance reduction using control variates and importance sampling for applications in computational statistical physics\nUrbain Vaes INRIA, CERMICS\nThe scaling of the mobility coefficient associated with two-dimensional Langevin dynamics in a periodic potential as the friction vanishes is not well understood. Theoretical results are lacking, and numerical calculation of the mobility in the underdamped regime is challenging. In the first part of this talk, I will present a new variance reduction approach based on control variates for efficiently estimating the mobility of Langevin-type dynamics, together with numerical experiments illustrating the performance of the approach.\nIn the second part of this talk, we study an importance sampling approach for calculating averages with respect to multimodal probability distributions. Traditional Markov chain Monte Carlo methods to this end, which are based on time averages along a realization of a Markov process ergodic with respect to the target probability distribution, are usually plagued by a large variance due to the metastability of the process. The estimator we study is based on an ergodic average along a realization of an overdamped Langevin process for a modified potential. We obtain an explicit expression for the optimal biasing potential in dimension 1 and propose a general numerical approach for approximating the optimal potential in the multi-dimensional setting."
  },
  {
    "objectID": "sem/s2024_02.html",
    "href": "sem/s2024_02.html",
    "title": "February 2024",
    "section": "",
    "text": "Date: 4PM Friday 16th February 2024\nRoom: Salle 07, PariSanté Campus\n\n\nSampling probability distributions on constrained spaces\nPierre Jacob ESSEC Business School\nI will describe the problem of designing MCMC samplers for probability distributions that are supported on submanifolds, and its relevance in statistics (hypothesis testing, Bayesian inference), and how one may implement couplings of such MCMC kernels. The question also arises when the target is supported in \\(R^d\\) and one wants to propose moves along the contour of the target density function, which could be well motivated, and has been suggested multiple times in the literature. Joint work with Elena Bortolato (Padova) and Robin Ryder (Paris-Dauphine).\n\n\nInsufficient Gibbs Sampling\nAntoine Luciano Ceremade, Université Paris Dauphine-PSL\nIn some applied scenarios, the availability of complete data is restricted, often due to privacy concerns, and only aggregated, robust and inefficient statistics derived from the data are accessible. These robust statistics are not sufficient, but they demonstrate reduced sensitivity to outliers and offer enhanced data protection due to their higher breakdown point. In this article, operating within a parametric framework, we propose a method to sample from the posterior distribution of parameters conditioned on different robust and inefficient statistics: specifically, the pairs (median, MAD) or (median, IQR), or one or more quantiles. Leveraging a Gibbs sampler and the simulation of latent augmented data, our approach facilitates simulation according to the posterior distribution of parameters belonging to specific families of distributions. We demonstrate its applicability on the Gaussian, Cauchy, and translated Weibull families.\n(Preprint: https://arxiv.org/abs/2307.14973)"
  },
  {
    "objectID": "sem/s2024_05.html",
    "href": "sem/s2024_05.html",
    "title": "May 2024",
    "section": "",
    "text": "Date: 4PM Friday 24th May 2024\nRoom: Salle 06, PariSanté Campus\n\n\nTitle TBC\nGabriel Stoltz CERMICS, Ecole des Ponts\nAbstract TBC\n\n\nSpeaker 2 TBC"
  },
  {
    "objectID": "psc_rooms.html",
    "href": "psc_rooms.html",
    "title": "Rooms at PariSanté",
    "section": "",
    "text": "Location: PariSanté Campus\n\nMore information on travelling to PariSanté and the facilities is available here."
  }
]